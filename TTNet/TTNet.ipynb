{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom TTNet Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from tqdm.notebook import tqdm\n",
    "from turbojpeg import TurboJPEG\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "GPU: NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "print('Device: ' + str(device))\n",
    "if use_cuda:\n",
    "    print('GPU: ' + str(torch.cuda.get_device_name(0)))\n",
    "\n",
    "# Turns on cuDNN Autotuner\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TTNet Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder paths\n",
    "dataset_base_path = './Dataset/images'\n",
    "savePath_base = \"./Trained_Models/\"\n",
    "outputPath = \"./Results/\"\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 1e-4\n",
    "eps = 1e-7\n",
    "sigma = 1\n",
    "event_num = 4 # Fly = 0, Bounce = 1, Hit = 2, Out = 3\n",
    "\n",
    "# Img resolutions\n",
    "data_width = 1920\n",
    "data_height = 1080 \n",
    "TTN_width=320\n",
    "TTN_height=128\n",
    "\n",
    "seed=42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "  A.RandomCrop(height=(int(data_height*0.85)), width=(int(data_width*0.85)), p=0.5),\n",
    "  A.Rotate(limit=15, p=0.5),\n",
    "  A.HorizontalFlip(p=0.5),\n",
    "  A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0, hue=0.1, p=0.5),\n",
    "  A.Resize(height=TTN_height, width=TTN_width, interpolation=1, always_apply=True, p=1)],\n",
    "  #A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=True, p=1.0)],\n",
    "  keypoint_params=A.KeypointParams(format='xy'),\n",
    "  additional_targets = {\n",
    "    'img2': 'image',\n",
    "    'img3': 'image',\n",
    "    'img4': 'image',\n",
    "    'img5': 'image',\n",
    "    'img6': 'image',\n",
    "    'img7': 'image',\n",
    "    'img8': 'image',\n",
    "    'img9': 'image',\n",
    "  })\n",
    "\n",
    "test_transform = A.Compose([\n",
    "  A.Resize(height=TTN_height, width=TTN_width, interpolation=1, always_apply=True, p=1)],\n",
    "  #A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=True, p=1.0)],\n",
    "  keypoint_params=A.KeypointParams(format='xy'),\n",
    "  additional_targets = {\n",
    "    'img2': 'image',\n",
    "    'img3': 'image',\n",
    "    'img4': 'image',\n",
    "    'img5': 'image',\n",
    "    'img6': 'image',\n",
    "    'img7': 'image',\n",
    "    'img8': 'image',\n",
    "    'img9': 'image',\n",
    "  })\n",
    "\n",
    "def smooth_event(events,event_num):\n",
    "  n = 5\n",
    "  is_found = False\n",
    "  for i in range((len(events)+1)//2):\n",
    "    middle_index =  (len(events))//2\n",
    "    if((int(events[middle_index+i])==event_num or int(events[middle_index-i])==event_num) and not is_found):\n",
    "      n = i\n",
    "      is_found = True\n",
    "  prob = np.cos(n * np.pi / 8)\n",
    "  if(prob<0.01):\n",
    "    return 0\n",
    "  else:\n",
    "    return prob\n",
    "\n",
    "class TTN_Dataset(Dataset):\n",
    "  def __init__(self, split, window_size=9):\n",
    "    self.split = split\n",
    "    self.window_size = window_size\n",
    "    self.window_paths = []\n",
    "    self.xy = []\n",
    "    self.event_probs = []\n",
    "    self.jpeg_reader = TurboJPEG()\n",
    "\n",
    "    game_list = os.listdir(f\"{dataset_base_path}/{split}\")\n",
    "    # Iterate through each game\n",
    "    for game in game_list:\n",
    "      game_dir = f\"{dataset_base_path}/{split}/{game}\"\n",
    "      clips = os.listdir(game_dir)\n",
    "      # Iterate through each clip\n",
    "      for clip in clips:\n",
    "        # Read and store annotation information\n",
    "        clip_dir = f\"{game_dir}/{clip}\"\n",
    "        annotation_path = f\"{clip_dir}/Annotation.csv\"\n",
    "        annotation = pd.read_csv(annotation_path)\n",
    "        img_paths = np.asarray(annotation.iloc[:, 0])\n",
    "        ball_x = np.asarray(annotation.iloc[:, 1])\n",
    "        ball_y = np.asarray(annotation.iloc[:, 2])\n",
    "        event = np.asarray(annotation.iloc[:, 3])\n",
    "\n",
    "        for frame_no, _ in enumerate(img_paths):\n",
    "          # Skip indices where it is not possible to make a frame window\n",
    "          if not (self.window_size-1 < frame_no <= len(img_paths)-1):\n",
    "            continue\n",
    "\n",
    "          # Get frame window image paths\n",
    "          window_path = []\n",
    "          middle_index = frame_no-self.window_size+(self.window_size+1)//2\n",
    "          for window_frame in range(self.window_size):\n",
    "            window_path.append(f\"{clip_dir}/{img_paths[(frame_no+1)-self.window_size+window_frame]}\")\n",
    "          self.window_paths.append(window_path)\n",
    "\n",
    "          # Get xy location\n",
    "          x = ball_x[middle_index]\n",
    "          y = ball_y[middle_index]\n",
    "          if x == 1920:\n",
    "            x -= 1\n",
    "          if y == 1080:\n",
    "            y -= 1\n",
    "          self.xy.append([(x, y)])\n",
    "\n",
    "          event_prob = torch.zeros((event_num))\n",
    "          for i in range(event_num):\n",
    "            event_prob[i] = smooth_event(event[frame_no+1-self.window_size:frame_no+1],i)\n",
    "          self.event_probs.append(event_prob)\n",
    "\n",
    "  def loader(self, start_index):\n",
    "    # Get images and keypoints\n",
    "    imgs = []\n",
    "    for i in range(9):\n",
    "      in_file = open(self.window_paths[start_index][i], 'rb')\n",
    "      image = self.jpeg_reader.decode(in_file.read(), 0)\n",
    "      imgs.append(image)\n",
    "    kp = self.xy[start_index]\n",
    "\n",
    "    # Apply transformation and get outputs\n",
    "    if self.split == \"train\":\n",
    "      transformed = train_transform(image=imgs[0], img2=imgs[1], img3=imgs[2], img4=imgs[3], img5=imgs[4], img6=imgs[5], img7=imgs[6], img8=imgs[7], img9=imgs[8], keypoints=kp)\n",
    "    else:\n",
    "      transformed = test_transform(image=imgs[0], img2=imgs[1], img3=imgs[2], img4=imgs[3], img5=imgs[4], img6=imgs[5], img7=imgs[6], img8=imgs[7], img9=imgs[8], keypoints=kp)\n",
    "    img1 = transformed['image']\n",
    "    img2 = transformed['img2']\n",
    "    img3 = transformed['img3']\n",
    "    img4 = transformed['img4']\n",
    "    img5 = transformed['img5']\n",
    "    img6 = transformed['img6']\n",
    "    img7 = transformed['img7']\n",
    "    img8 = transformed['img8']\n",
    "    img9 = transformed['img9']\n",
    "    xy_downscale = transformed['keypoints']\n",
    "    xy_downscale = xy_downscale[0] if xy_downscale else (0, 0)\n",
    "\n",
    "    return img1, img2, img3, img4, img5, img6, img7, img8, img9, xy_downscale\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    img1, img2, img3, img4, img5, img6, img7, img8, img9, xy_downscale = self.loader(index)\n",
    "    window_imgs = np.concatenate((img1, img2, img3, img4, img5, img6, img7, img8, img9), axis=2)\n",
    "    window_imgs = np.rollaxis(window_imgs, 2, 0)\n",
    "    window_imgs = torch.from_numpy(window_imgs)\n",
    "\n",
    "    centre_path = self.window_paths[index][self.window_size//2]\n",
    "    event_probs = self.event_probs[index]\n",
    "\n",
    "    return window_imgs, xy_downscale, centre_path, event_probs\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.window_paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Train...\n",
      "Loading Test...\n",
      "Dataset total batches:\n",
      "Training : 632\n",
      "Test : 64\n"
     ]
    }
   ],
   "source": [
    "train_batch = 64\n",
    "test_batch = 64\n",
    "num_workers = 16\n",
    "\n",
    "print(\"Loading Train...\")\n",
    "train_loader = DataLoader(TTN_Dataset(split=\"train\"), batch_size=train_batch,shuffle=True,num_workers=num_workers,pin_memory=True,drop_last=True)\n",
    "print(\"Loading Test...\")\n",
    "test_loader = DataLoader(TTN_Dataset(split=\"test\"), batch_size=test_batch,shuffle=False,num_workers=num_workers,pin_memory=True)\n",
    "print(\"Dataset total batches:\")\n",
    "print(f\"Training : {len(train_loader)}\")\n",
    "print(f\"Test : {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO This entire section probably needs a rewrite\n",
    "def gaussian_1d(pos, mu, sigma):\n",
    "  target = torch.exp(- (((pos - mu) / sigma) ** 2) / 2)\n",
    "  return target\n",
    "\n",
    "def ball_loss(pre_output,true_label):\n",
    "  x_pred = pre_output[:, :TTN_width]\n",
    "  y_pred = pre_output[:, TTN_width:]\n",
    "\n",
    "  target_output = torch.zeros_like(pre_output)\n",
    "  for i in range(pre_output.shape[0]):\n",
    "    target_output_temp = torch.zeros((TTN_width+TTN_height), device=device)\n",
    "    x,y = true_label[i]\n",
    "    if (TTN_width > x > 0) and (TTN_height > y > 0):    \n",
    "      x_axis = torch.arange(0, TTN_width, device=device)\n",
    "      y_axis = torch.arange(0, TTN_height, device=device)\n",
    "\n",
    "      target_output_temp[:TTN_width] = gaussian_1d(x_axis, x, sigma=sigma)\n",
    "      target_output_temp[TTN_width:] = gaussian_1d(y_axis, y, sigma=sigma)\n",
    "      target_output_temp[target_output_temp < 1e-2] = 0.\n",
    "    target_output[i] = target_output_temp\n",
    "    \n",
    "  x_target = target_output[:, :TTN_width]\n",
    "  y_target = target_output[:, TTN_width:]\n",
    "\n",
    "  loss_x = - torch.mean(x_target * torch.log(x_pred + eps) + (1 - x_target) * torch.log(1 - x_pred + eps))\n",
    "  loss_y = - torch.mean(y_target * torch.log(y_pred + eps) + (1 - y_target) * torch.log(1 - y_pred + eps))\n",
    "  loss = loss_x+loss_y\n",
    "  return loss\n",
    "\n",
    "def event_loss(pre_output,true_label):\n",
    "  weights = (1, 57, 40, 3)\n",
    "  weights = torch.tensor(weights).to(device)\n",
    "  weights = weights.view(-1,weights.shape[0])\n",
    "  weights = weights / weights.sum()\n",
    "  loss = -torch.mean(weights * (true_label * torch.log(pre_output + eps) + (1. - true_label) * torch.log(1 - pre_output + eps)))\n",
    "  return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTNet Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Block Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels):\n",
    "    super(ConvBlock, self).__init__() \n",
    "    self.block = nn.Sequential(\n",
    "      nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),          \n",
    "      nn.BatchNorm2d(out_channels, track_running_stats=False),\n",
    "      nn.ReLU(inplace=True),\n",
    "      nn.MaxPool2d(kernel_size=2, stride=2, padding=0, ceil_mode=True),\n",
    "    )\n",
    "    \n",
    "  def forward(self, x):\n",
    "    out = self.block(x)\n",
    "    return out\n",
    "\n",
    "class ConvBlock_without_Pooling(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels):\n",
    "    super(ConvBlock_without_Pooling, self).__init__() \n",
    "    self.block = nn.Sequential(\n",
    "      nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "      nn.BatchNorm2d(out_channels),\n",
    "      nn.ReLU(),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.block(x)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BallDetection(nn.Module):\n",
    "  def __init__(self, frame_window, dropout_p):\n",
    "    super(BallDetection, self).__init__()\n",
    "    self.convBlocks = nn.Sequential(\n",
    "      nn.Conv2d(in_channels = frame_window*3, out_channels=64, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "      nn.BatchNorm2d(64,track_running_stats=False),\n",
    "      nn.ReLU(inplace=True),\n",
    "      ConvBlock(in_channels=64, out_channels=64),\n",
    "      ConvBlock(in_channels=64, out_channels=64),\n",
    "      nn.Dropout2d(p=dropout_p),\n",
    "      ConvBlock(in_channels=64, out_channels=128),\n",
    "      ConvBlock(in_channels=128, out_channels=128),\n",
    "      ConvBlock(in_channels=128, out_channels=256),\n",
    "      ConvBlock(in_channels=256, out_channels=256),\n",
    "    )\n",
    "    self.FC = nn.Sequential(\n",
    "      nn.Linear(in_features=2560, out_features=1792),\n",
    "      nn.ReLU(inplace=True),\n",
    "      nn.Dropout(p=dropout_p),\n",
    "      nn.Linear(in_features=1792, out_features=896),\n",
    "      nn.ReLU(inplace=True),\n",
    "      nn.Dropout(p=dropout_p),\n",
    "      nn.Linear(in_features=896, out_features=int(TTN_width+TTN_height)),\n",
    "      nn.Sigmoid(),\n",
    "    )\n",
    "    self.dropout2d = nn.Dropout2d(p=dropout_p)\n",
    "\n",
    "  def forward(self, x):\n",
    "    block6_out = self.convBlocks(x)\n",
    "    x = self.dropout2d(block6_out)\n",
    "    x = x.contiguous().view(x.shape[0], -1)\n",
    "    out = self.FC(x)\n",
    "    return out, block6_out\n",
    "\n",
    "class EventSpotting(nn.Module):\n",
    "  def __init__(self, dropout_p):\n",
    "    super(EventSpotting, self).__init__()\n",
    "    self.convBlocks = nn.Sequential(\n",
    "      nn.Conv2d(512, 64, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "      nn.BatchNorm2d(64),\n",
    "      nn.ReLU(),\n",
    "      nn.Dropout2d(p=dropout_p),\n",
    "      ConvBlock_without_Pooling(in_channels=64, out_channels=64),\n",
    "      nn.Dropout2d(p=dropout_p),\n",
    "      ConvBlock_without_Pooling(in_channels=64, out_channels=64),\n",
    "      nn.Dropout2d(p=dropout_p),\n",
    "    )\n",
    "    self.FC = nn.Sequential(           \n",
    "      nn.Linear(in_features=640, out_features=512),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(in_features=512, out_features=event_num),\n",
    "      nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "  def forward(self, global_features, local_features):\n",
    "    x = torch.cat((global_features, local_features), dim=1)\n",
    "    x = self.convBlocks(x)\n",
    "    x = x.contiguous().view(x.size(0), -1)\n",
    "    out = self.FC(x)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TTNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTNet(nn.Module):\n",
    "  def __init__(self, dropout_p, frame_window, threshold, tasks, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "    super(TTNet, self).__init__() \n",
    "    # Assign stages\n",
    "    self.local_stage,self.event_spotting = None,None\n",
    "    self.global_stage = BallDetection(frame_window, dropout_p)\n",
    "    if \"local\" in tasks:\n",
    "      self.local_stage = BallDetection(frame_window, dropout_p)\n",
    "    if \"event\" in tasks:\n",
    "      self.event_spotting = EventSpotting(dropout_p)\n",
    "\n",
    "    self.threshold = threshold\n",
    "    self.mean = torch.repeat_interleave(torch.tensor(mean).view(1, 3, 1, 1), repeats=9, dim=1)\n",
    "    self.std = torch.repeat_interleave(torch.tensor(std).view(1, 3, 1, 1), repeats=9, dim=1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    local_out, local_in, crop_params, event_out = None, None, None, None\n",
    "\n",
    "    global_out, global_features = self.global_stage(x)\n",
    "    if self.local_stage is not None:\n",
    "      local_in, crop_params = self.crop_imgs(x, global_out)\n",
    "      local_out, local_features = self.local_stage(local_in)\n",
    "      if self.event_spotting is not None:\n",
    "        event_out = self.event_spotting(global_features, local_features)\n",
    "    \n",
    "    return global_out, local_out, local_in, crop_params, event_out\n",
    "\n",
    "  # TODO This section needs a rewrite\n",
    "  def crop_imgs(self, x, global_xy):\n",
    "    global_xy_copy = global_xy.detach().clone()\n",
    "    global_xy_copy[global_xy_copy < self.threshold] = 0\n",
    "    crop_params = []\n",
    "    ball_detected = False\n",
    "\n",
    "    global_output = torch.zeros_like(x)\n",
    "    # original size\n",
    "    original_input = F.interpolate(x, (data_height, data_width))\n",
    "    \n",
    "    for i in range(x.shape[0]):\n",
    "      pos_x = global_xy_copy[i, :TTN_width]\n",
    "      pos_y = global_xy_copy[i, TTN_width:]\n",
    "\n",
    "      if (torch.sum(pos_x) == 0) or (torch.sum(pos_y) == 0):\n",
    "        x_center = int(TTN_width / 2)\n",
    "        y_center = int(TTN_height / 2)\n",
    "      else:\n",
    "        x_center = torch.argmax(pos_x)\n",
    "        y_center = torch.argmax(pos_y)\n",
    "        ball_detected = True\n",
    "\n",
    "      # original size\n",
    "      x_center = int(x_center * (data_width/TTN_width))\n",
    "      y_center = int(y_center * (data_height/TTN_height))\n",
    "      x_min = max(0, x_center - int(TTN_width / 2))\n",
    "      y_min = max(0, y_center - int(TTN_height / 2))\n",
    "      x_max = min(data_width, x_min + TTN_width)\n",
    "      y_max = min(data_height, y_min + TTN_height)\n",
    "      crop_width = x_max - x_min\n",
    "      crop_height = y_max - y_min\n",
    "      padding_x=padding_y=0\n",
    "      if (crop_height != TTN_height) or (crop_width != TTN_width):\n",
    "        padding_x = int((TTN_width - crop_width) / 2)\n",
    "        padding_y = int((TTN_height - crop_height) / 2)\n",
    "        global_output[i, :, padding_y:(padding_y + crop_height), padding_x:(padding_x + crop_width)] = original_input[i, :,y_min:y_max, x_min: x_max]\n",
    "      else:\n",
    "        global_output[i, :, :, :] = original_input[i, :, y_min:y_max, x_min: x_max]\n",
    "      crop_params.append([ball_detected,x_min,y_min,x_max,y_max,padding_x,padding_y])\n",
    "\n",
    "\n",
    "    return global_output,crop_params\n",
    "\n",
    "  def norm(self,x):\n",
    "    if not self.mean.is_cuda:\n",
    "      self.mean = self.mean.cuda()\n",
    "      self.std = self.std.cuda()\n",
    "\n",
    "    return (x / 255. - self.mean) / self.std\n",
    "\n",
    "def freeze_model(model, freeze_list):\n",
    "  for layer_name, p in model.named_parameters():\n",
    "    p.requires_grad = True\n",
    "    for freeze_module in freeze_list:\n",
    "      if freeze_module in layer_name:\n",
    "        p.requires_grad = False\n",
    "        break\n",
    "  print(\"Frozen\")\n",
    "  return model\n",
    "\n",
    "# TODO Rewrite this\n",
    "def get_local_groundtruth(global_ball_pos_xyz,crop_params):\n",
    "  \n",
    "  local_ball_pos_xyz = []\n",
    "  for i,para in enumerate(crop_params):\n",
    "    ball_detected,x_min,y_min,x_max,y_max,padding_x,padding_y = para\n",
    "    if ball_detected:\n",
    "      ori_x = global_ball_pos_xyz[0][i].item()/TTN_width*data_width\n",
    "      ori_y = global_ball_pos_xyz[1][i].item()/TTN_height*data_height\n",
    "      local_x = max(ori_x - x_min + padding_x, -1)\n",
    "      local_y = max(ori_y - y_min + padding_y, -1)\n",
    "\n",
    "      if not (TTN_width>local_x>=0 and TTN_height>local_y>=0):\n",
    "        local_x = local_y = -1\n",
    "     \n",
    "    else:\n",
    "      local_x = local_y = -1\n",
    "    local_ball_pos_xyz.append([local_x,local_y])\n",
    "\n",
    "  return local_ball_pos_xyz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traning Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training config\n",
    "model_saved = True\n",
    "ph1_epochs = 30\n",
    "global_weight = 1\n",
    "ph1_train_loss_log = []\n",
    "ph1_test_loss_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "tasks = []\n",
    "model = TTNet(dropout_p=0.5, frame_window=9, threshold=0.01, tasks=tasks).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "lr_scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "if (model_saved):\n",
    "  savePath = f\"{savePath_base}Phase1/TTNet_Phase1_22.pth\"\n",
    "  print(f\"Loading model from path: {savePath}\")\n",
    "  checkpoint = torch.load(savePath)\n",
    "  model.load_state_dict(checkpoint['model'])\n",
    "  optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "  lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "  start_epoch = checkpoint['cur_epoch']\n",
    "  ph1_train_loss_log = checkpoint['train_loss_log']\n",
    "  ph1_test_loss_log = checkpoint['val_loss_log']\n",
    "  print(f\"Load phase 1 at epoch {start_epoch} succeed\")\n",
    "else:\n",
    "  start_epoch = 0\n",
    "  print(\"Phase 1: No model to load, start to train at epoch 0\")\n",
    "\n",
    "print(\"START TO TRAIN PHASE 1: Global Stage ...\")\n",
    "for epoch in range(start_epoch+1, ph1_epochs + 1):\n",
    "  model.train()\n",
    "  batch_num = len(train_loader)\n",
    "  train_loss = 0\n",
    "  train_loss_total = 0\n",
    "\n",
    "  with tqdm(train_loader, unit=\"batch\") as trepoch:\n",
    "    for data_batch in trepoch:\n",
    "      trepoch.set_description(f\"Train Epoch {epoch}\")\n",
    "      # Read in train batch\n",
    "      window_imgs, xy_downscale, _, _ = data_batch\n",
    "      window_batch = torch.as_tensor(window_imgs,dtype=torch.float32).to(device)\n",
    "      xy_downscale=torch.stack((xy_downscale[0],xy_downscale[1])).transpose(0,1)\n",
    "      \n",
    "      # Calculate train (global) loss\n",
    "      with autocast():\n",
    "        global_out, _, _, _, _ = model(window_batch)\n",
    "        train_loss = ball_loss(global_out,xy_downscale) * global_weight\n",
    "\n",
    "      optimizer.zero_grad(set_to_none=True)\n",
    "      scaler.scale(train_loss).backward()\n",
    "      loss = train_loss.detach().cpu().numpy()\n",
    "      \n",
    "      train_loss_total += loss\n",
    "      scaler.step(optimizer)\n",
    "      scaler.update()\n",
    "      trepoch.set_postfix(loss=loss)\n",
    "\n",
    "  # Log training losses\n",
    "  tqdm.write(f\"Train\\t epoch: {epoch}/{ph1_epochs}\\t loss: {train_loss_total/batch_num}\")\n",
    "  ph1_train_loss_log.append(train_loss_total/batch_num)\n",
    "\n",
    "  model.eval()\n",
    "  with torch.no_grad():     \n",
    "    batch_num = len(test_loader)\n",
    "    test_loss = 0\n",
    "    test_loss_total = 0\n",
    "    with tqdm(test_loader, unit=\"batch\") as tepoch:\n",
    "      for data_batch in tepoch:\n",
    "        tepoch.set_description(f\"Test Epoch {epoch}\")\n",
    "        # Read in test batch\n",
    "        window_imgs ,xy_downscale, _,_ = data_batch\n",
    "        window_batch = torch.as_tensor(window_imgs,dtype=torch.float32).to(device)      \n",
    "        xy_downscale=torch.stack((xy_downscale[0],xy_downscale[1])).transpose(0,1)\n",
    "\n",
    "        # Model forward step and calculate test (global) loss\n",
    "        with autocast():\n",
    "          global_out, _, _, _, _ = model(window_batch)\n",
    "          test_loss = ball_loss(global_out,xy_downscale) * global_weight\n",
    "        \n",
    "        loss = test_loss.detach().cpu().numpy()\n",
    "        test_loss_total += loss\n",
    "        tepoch.set_postfix(loss=loss)\n",
    "\n",
    "    # Log test losses\n",
    "    ph1_test_loss_log.append(test_loss_total/batch_num)\n",
    "    tqdm.write(f\"Test\\t epoch: {epoch}/{ph1_epochs}\\t loss: {test_loss_total/batch_num}\")\n",
    "  \n",
    "  tqdm.write(\"Saving model\")\n",
    "  state = {'model':model.state_dict(),'optimizer':optimizer.state_dict(),'lr_scheduler':lr_scheduler.state_dict(),'cur_epoch':epoch,'train_loss_log':ph1_train_loss_log,'val_loss_log':ph1_test_loss_log}\n",
    "  savePath = f\"{savePath_base}Phase1/TTNet_Phase1_{epoch}.pth\"\n",
    "  torch.save(state, savePath)\n",
    "  model_saved = True\n",
    "\n",
    "  # Plot the losses\n",
    "  plt.clf()\n",
    "  plt.figure(dpi=1200)\n",
    "  plt.plot(range(1, epoch+1),ph1_train_loss_log,label='Train loss')\n",
    "  plt.plot(range(1, epoch+1),ph1_test_loss_log,label='Test loss')\n",
    "  plt.legend()\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.title('Training Loss')\n",
    "  plt.savefig(outputPath+\"loss_ph1.png\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phase1_test():\n",
    "  model.eval()\n",
    "  batch_num = len(test_loader)\n",
    "  dist = 0\n",
    "  dist_x = 0\n",
    "  dist_y = 0\n",
    "  count = 0\n",
    "  avg_dists = []\n",
    "  x_dists = []\n",
    "  y_dists = []\n",
    "  with torch.no_grad(): \n",
    "    with tqdm(test_loader, unit=\"batch\") as tepoch:\n",
    "      for data_batch in tepoch:\n",
    "        tepoch.set_description(f\"Test\")\n",
    "        # Read in test batch\n",
    "        window_imgs, xy_downscale, window_centre, _ = data_batch\n",
    "        window_batch = torch.as_tensor(window_imgs,dtype=torch.float32).to(device)\n",
    "        xy_downscale_batch=torch.stack((xy_downscale[0],xy_downscale[1])).transpose(0,1)\n",
    "\n",
    "        with autocast():\n",
    "          global_out, _, _, _, _ = model(window_batch)\n",
    "        \n",
    "        global_out_clone = global_out.clone().detach()\n",
    "        for i in range(global_out.shape[0]):\n",
    "          global_output_clone_x = global_out_clone[i,:TTN_width]\n",
    "          global_output_clone_y = global_out_clone[i,TTN_width:TTN_width+TTN_height]\n",
    "          global_output_x = torch.argmax(global_output_clone_x).item()\n",
    "          global_output_y = torch.argmax(global_output_clone_y).item()\n",
    "\n",
    "          pred_x = global_output_x*data_width/TTN_width\n",
    "          pred_y = global_output_y*data_height/TTN_height\n",
    "\n",
    "          pred_x = 0 if pred_x<0 else pred_x\n",
    "          pred_y = 0 if pred_y<0 else pred_y\n",
    "          pred = np.array([int(pred_x), int(pred_y)])\n",
    "\n",
    "          ori_x = xy_downscale[0][i].item()*(data_width/TTN_width)\n",
    "          ori_y = xy_downscale[1][i].item()*(data_height/TTN_height)\n",
    "          ori = np.array([int(ori_x), int(ori_y)])\n",
    "\n",
    "          dist += np.linalg.norm(ori-pred)\n",
    "          dist_x += np.abs(ori_x - pred_x)\n",
    "          dist_y += np.abs(ori_y - pred_y)\n",
    "          count += 1\n",
    "            \n",
    "          centre_img = cv.imread(window_centre[i])\n",
    "          cv.circle(centre_img, (int(pred_x),int(pred_y)), 8,  (0, 0, 255), 2)\n",
    "          #out.write(centre_img)\n",
    "    \n",
    "    avg_dists.append(dist/count)\n",
    "    x_dists.append(dist_x/count)\n",
    "    y_dists.append(dist_y/count)\n",
    "    \n",
    "    print(f\"Avg Dist = {dist/count}\")\n",
    "    print(f\"X Dist = {dist_x/count}\")\n",
    "    print(f\"Y Dist = {dist_y/count}\")\n",
    "  return avg_dists, x_dists, y_dists\n",
    "    \n",
    "global_weight = 1\n",
    "for i in range(1, 31):\n",
    "    saved_epoch = i\n",
    "    tasks = []\n",
    "    model = TTNet(dropout_p=0.5, frame_window=9, threshold=0.01, tasks=tasks).to(device)\n",
    "    savePath = f\"{savePath_base}Phase1/TTNet_Phase1_{i}.pth\"\n",
    "    checkpoint = torch.load(savePath)\n",
    "    model.load_state_dict(checkpoint['model']) \n",
    "    ph1_train_loss_log = checkpoint['train_loss_log']\n",
    "    ph1_test_loss_log = checkpoint['val_loss_log']\n",
    "    print(f\"Epoch {saved_epoch}\")\n",
    "    avg_dists, x_dists, y_dists = phase1_test()\n",
    "    print(f\"Train loss\\t : {ph1_train_loss_log[saved_epoch-1]}\")\n",
    "    print(f\"Test loss\\t : {ph1_test_loss_log[saved_epoch-1]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training config\n",
    "ph2_saved = False\n",
    "ph2_epochs = 30\n",
    "local_weight = 1\n",
    "event_weight = 2\n",
    "ph2_train_loss_log = []\n",
    "ph2_train_loc_log = []\n",
    "ph2_train_event_log = []\n",
    "ph2_test_loss_log = []\n",
    "ph2_test_loc_log = []\n",
    "ph2_test_event_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_list = [\"global_stage\"]\n",
    "tasks = [\"local\", \"event\"]\n",
    "model = TTNet(dropout_p=0.5, frame_window=9, threshold=0.01, tasks=tasks).to(device)\n",
    "model = freeze_model(model,freeze_list)                           \n",
    "train_params = [param for param in model.parameters() if param.requires_grad]\n",
    "optimizer = torch.optim.AdamW(train_params,lr=learning_rate)\n",
    "lr_scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# TODO Rewrite this\n",
    "def load_weights_local_stage(pretrained_dict):\n",
    "  local_weights_dict = {}\n",
    "  for layer_name, v in pretrained_dict.items():\n",
    "    if 'global_stage' in layer_name:\n",
    "      layer_name_parts = layer_name.split('.')\n",
    "      layer_name_parts[1] = 'local_stage'\n",
    "      local_name = '.'.join(layer_name_parts)\n",
    "      local_weights_dict[local_name] = v\n",
    "\n",
    "  return {**pretrained_dict, **local_weights_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ph2_saved:\n",
    "  savePath = f\"{savePath_base}Phase2/TTNet_Phase2_1.pth\"\n",
    "  print(\"Loading model from path: \")\n",
    "  print(savePath)\n",
    "  checkpoint = torch.load(savePath)\n",
    "  model.load_state_dict(checkpoint['model'])\n",
    "  optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "  lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "  start_epoch = checkpoint['cur_epoch']\n",
    "  ph2_train_loss_log = checkpoint['train_loss_log']\n",
    "  ph2_test_loss_log = checkpoint['val_loss_log']\n",
    "  print(f\"Load phase 2 at epoch {start_epoch} succeed\")\n",
    "else:\n",
    "  checkpoint = torch.load(savePath_base+\"Phase1/TTNet_Phase1_30.pth\", map_location='cpu')\n",
    "  pretrained_dict = checkpoint['model']\n",
    "  model_state_dict = model.state_dict()\n",
    "  pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_state_dict}\n",
    "  pretrained_dict = load_weights_local_stage(pretrained_dict)\n",
    "  model_state_dict.update(pretrained_dict)\n",
    "  model.load_state_dict(model_state_dict, strict=False)\n",
    "  model = model.to(device)\n",
    "\n",
    "  start_epoch = 0\n",
    "  print(\"Phase 2: No model to load, start to train at epoch 0\")\n",
    "\n",
    "print('START TO TRAIN PHASE 2: Local + Event Stage ...')\n",
    "for epoch in range(start_epoch+1, ph2_epochs+1):\n",
    "  model.train()\n",
    "  batch_num = len(train_loader)\n",
    "  train_loss = 0\n",
    "  train_loss_total = 0\n",
    "  tr_loc_loss_total = 0\n",
    "  tr_event_loss_total = 0\n",
    "\n",
    "  with tqdm(train_loader, unit=\"batch\") as trepoch:\n",
    "    for data_batch in trepoch:\n",
    "      trepoch.set_description(f\"Train Epoch {epoch}\")\n",
    "      # Read in train batch\n",
    "      window_imgs, xy_downscale, _, event_probs = data_batch\n",
    "      window_batch = torch.as_tensor(window_imgs,dtype=torch.float32).to(device, non_blocking=True)\n",
    "\n",
    "      # Model forward step\n",
    "      with autocast():\n",
    "        _, local_out, _, crop_params, event_out = model(window_batch)\n",
    "\n",
    "      # Get event probs and local xy predictions\n",
    "      event_probs = torch.as_tensor(event_probs,dtype=torch.float32).to(device, non_blocking=True)\n",
    "      local_ball_xy = get_local_groundtruth(xy_downscale,crop_params)\n",
    "      local_ball_xy = torch.as_tensor(local_ball_xy,dtype=torch.float32).to(device)\n",
    "\n",
    "      # Calucate train (local+event) loss\n",
    "      with autocast():\n",
    "        local_loss_train = ball_loss(local_out,local_ball_xy) * local_weight\n",
    "        event_loss_train = event_loss(event_out,event_probs) * event_weight\n",
    "        train_loss = local_loss_train + event_loss_train\n",
    "      \n",
    "      optimizer.zero_grad()\n",
    "      scaler.scale(train_loss).backward()\n",
    "      loss = train_loss.detach().cpu().numpy()\n",
    "      loc = local_loss_train.detach().cpu().numpy()\n",
    "      event = event_loss_train.detach().cpu().numpy()\n",
    "      \n",
    "      train_loss_total += loss\n",
    "      tr_loc_loss_total += loc\n",
    "      tr_event_loss_total += event\n",
    "\n",
    "      scaler.step(optimizer)\n",
    "      scaler.update()\n",
    "      trepoch.set_postfix(loss=loss)\n",
    "\n",
    "  # Log training losses\n",
    "  tqdm.write(f\"Train\\t epoch: {epoch}/{ph2_epochs}\\t loss: {train_loss_total/batch_num} Local : {tr_loc_loss_total/batch_num} Event {tr_event_loss_total/batch_num}\")\n",
    "  ph2_train_loss_log.append(train_loss_total/batch_num)\n",
    "  ph2_train_loc_log.append(tr_loc_loss_total/batch_num)\n",
    "  ph2_train_event_log.append(tr_event_loss_total/batch_num)\n",
    "\n",
    "  model.eval()\n",
    "  with torch.no_grad():   \n",
    "    batch_num = len(test_loader)\n",
    "    test_loss = 0\n",
    "    test_loss_total = 0\n",
    "    test_loc_loss_total = 0\n",
    "    test_event_loss_total = 0\n",
    "    with tqdm(test_loader, unit=\"batch\") as tepoch:\n",
    "      for data_batch in tepoch:\n",
    "        tepoch.set_description(f\"Test Epoch {epoch}\")\n",
    "        # Read in test batch\n",
    "        window_imgs, xy_downscale, _, event_probs = data_batch\n",
    "        window_batch = torch.as_tensor(window_imgs,dtype=torch.float32).to(device, non_blocking=True)\n",
    "\n",
    "        # Model forward step\n",
    "        with autocast():\n",
    "          _, local_out, _, crop_params, event_out = model(window_batch)\n",
    "\n",
    "        # Get event probs and local xy predictions\n",
    "        event_probs = torch.as_tensor(event_probs,dtype=torch.float32).to(device, non_blocking=True)\n",
    "        local_ball_xy = get_local_groundtruth(xy_downscale,crop_params)\n",
    "        local_ball_xy = torch.as_tensor(local_ball_xy,dtype=torch.float32).to(device)\n",
    "\n",
    "        # Calucate test (local+event) loss\n",
    "        with autocast():\n",
    "          local_loss_test = ball_loss(local_out,local_ball_xy) * local_weight\n",
    "          event_loss_test = event_loss(event_out,event_probs) * event_weight\n",
    "          test_loss = local_loss_test + event_loss_test\n",
    "\n",
    "        loss = test_loss.detach().cpu().numpy()\n",
    "        loc = local_loss_test.detach().cpu().numpy()\n",
    "        event = event_loss_test.detach().cpu().numpy()\n",
    "\n",
    "        test_loss_total += loss\n",
    "        test_loc_loss_total += loc\n",
    "        test_event_loss_total += event\n",
    "        tepoch.set_postfix(loss=loss)\n",
    "\n",
    "    ph2_test_loss_log.append(test_loss_total/batch_num)\n",
    "    ph2_test_loc_log.append(test_loc_loss_total/batch_num)\n",
    "    ph2_test_event_log.append(test_event_loss_total/batch_num)\n",
    "    tqdm.write(f\"Test\\t epoch: {epoch}/{ph2_epochs}\\t loss: {test_loss_total/batch_num} Local : {test_loc_loss_total/batch_num} Event {test_event_loss_total/batch_num}\")\n",
    "\n",
    "  tqdm.write(\"Saving model\")\n",
    "  savePath = f\"{savePath_base}/Phase2/TTNet_Phase2_{epoch}.pth\"\n",
    "  state = {'model':model.state_dict(),'optimizer':optimizer.state_dict(),'lr_scheduler':lr_scheduler.state_dict(),'cur_epoch':epoch,'train_loss_log':ph2_train_loss_log,'val_loss_log':ph2_test_loss_log}\n",
    "  torch.save(state, savePath) \n",
    "  ph2_saved = True\n",
    "\n",
    "  # Plot the losses\n",
    "  plt.clf()\n",
    "  plt.plot(range(1, epoch+1),ph2_train_loss_log,label='Train loss')\n",
    "  #plt.plot(range(epoch-1, epoch+len(ph2_train_loc_log)-1),ph2_train_loc_log,label='Train Local loss')\n",
    "  #plt.plot(range(epoch-1, epoch+len(ph2_train_event_log)-1),ph2_train_event_log,label='Train Event loss')\n",
    "  #plt.plot(range(epoch-1, epoch+len(ph2_val_loc_log)-1),ph2_val_loc_log,label='Validation Local loss')\n",
    "  #plt.plot(range(epoch-1, epoch+len(ph2_val_event_log)-1),ph2_val_event_log,label='Validation Event loss')\n",
    "  plt.plot(range(1, epoch+1),ph2_test_loss_log,label='Test loss')\n",
    "  plt.legend()\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.title('Training Loss')\n",
    "  plt.savefig(outputPath+\"loss_ph2.png\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Phase 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training config\n",
    "ph3_saved = False\n",
    "ph3_epochs = 20\n",
    "global_weight = 1\n",
    "local_weight = 1\n",
    "event_weight = 1\n",
    "ph3_train_loss_log = []\n",
    "ph3_tr_global_log = []\n",
    "ph3_tr_local_log = []\n",
    "ph3_tr_event_log = []\n",
    "\n",
    "ph3_test_loss_log = []\n",
    "ph3_test_global_log = []\n",
    "ph3_test_local_log = []\n",
    "ph3_test_event_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [\"local\", \"event\"]\n",
    "model = TTNet(dropout_p=0.5, frame_window=9, threshold=0.01, tasks=tasks).to(device)\n",
    "train_params = [param for param in model.parameters() if param.requires_grad]\n",
    "optimizer = torch.optim.AdamW(train_params,lr=learning_rate)\n",
    "lr_scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ph3_saved:\n",
    "  savePath = savePath_base+\"Phase3/TTNet_Phase3_15.pth\"\n",
    "  print(\"Loading model from path: \")\n",
    "  print(savePath)\n",
    "  checkpoint = torch.load(savePath)\n",
    "  model.load_state_dict(checkpoint['model'])\n",
    "  optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "  lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "  start_epoch = checkpoint['cur_epoch']\n",
    "  ph3_train_loss_log = checkpoint['train_loss_log']\n",
    "  ph3_test_loss_log = checkpoint['val_loss_log']\n",
    "  print(f\"Load phase 3 at epoch {start_epoch} succeed\")\n",
    "else:\n",
    "  checkpoint = torch.load(savePath_base+\"Phase2/TTNet_Phase2_25.pth\", map_location='cpu')\n",
    "  pretrained_dict = checkpoint['model']\n",
    "  model_state_dict = model.state_dict()\n",
    "  pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_state_dict}\n",
    "  model_state_dict.update(pretrained_dict)\n",
    "  model.load_state_dict(model_state_dict,strict=False)\n",
    "  model = model.to(device)\n",
    "\n",
    "  start_epoch = 0\n",
    "  print(\"Phase 3: No model to load, start to train at epoch 0\")\n",
    "\n",
    "print('START TO TRAIN PHASE 3: All Stages ...')\n",
    "\n",
    "for epoch in range(start_epoch+1, ph3_epochs+1):\n",
    "  model.train()\n",
    "  batch_num = len(train_loader)\n",
    "  train_loss = 0\n",
    "  train_loss_total = 0\n",
    "  tr_global_total = 0\n",
    "  tr_local_total = 0\n",
    "  tr_event_total = 0\n",
    "\n",
    "  with tqdm(train_loader, unit=\"batch\") as trepoch:\n",
    "    for data_batch in trepoch:\n",
    "      trepoch.set_description(f\"Train Epoch {epoch}\")\n",
    "      # Read in train batch\n",
    "      window_imgs, xy_downscale, _, event_probs = data_batch\n",
    "      window_batch = torch.as_tensor(window_imgs,dtype=torch.float32).to(device, non_blocking=True)\n",
    "      xy_downscale_batch = torch.stack((xy_downscale[0],xy_downscale[1])).transpose(0,1)\n",
    "\n",
    "      # Model forward step\n",
    "      with autocast():\n",
    "        global_out, local_out, _, crop_params, event_out = model(window_batch)\n",
    "      \n",
    "      # Get event probs and local xy predictions\n",
    "      event_probs = torch.as_tensor(event_probs,dtype=torch.float32).to(device, non_blocking=True)\n",
    "      local_ball_xy = get_local_groundtruth(xy_downscale,crop_params)\n",
    "      local_ball_xy = torch.as_tensor(local_ball_xy,dtype=torch.float32).to(device)\n",
    "\n",
    "      # Calucate train (global+local+event) loss\n",
    "      with autocast():\n",
    "        global_loss_train = ball_loss(global_out,xy_downscale_batch) * global_weight\n",
    "        local_loss_train = ball_loss(local_out,local_ball_xy) * local_weight\n",
    "        event_loss_train = event_loss(event_out,event_probs) * event_weight\n",
    "        train_loss = global_loss_train + local_loss_train + event_loss_train\n",
    "\n",
    "      optimizer.zero_grad(set_to_none=True)\n",
    "      scaler.scale(train_loss).backward()\n",
    "      loss = train_loss.detach().cpu().numpy()\n",
    "      glo = global_loss_train.detach().cpu().numpy()\n",
    "      local = local_loss_train.detach().cpu().numpy()\n",
    "      event = event_loss_train.detach().cpu().numpy()\n",
    "        \n",
    "      train_loss_total += loss\n",
    "      tr_global_total += glo\n",
    "      tr_local_total += local\n",
    "      tr_event_total += event\n",
    "\n",
    "      scaler.step(optimizer)\n",
    "      scaler.update()\n",
    "      trepoch.set_postfix(loss=loss)\n",
    "\n",
    "    #train_loss.backward()\n",
    "    #optimizer.step()\n",
    "    #train_loss_total += train_loss.item()\n",
    "\n",
    "  # Log trainig losses\n",
    "  tqdm.write(f\"Train\\t epoch: {epoch}/{ph3_epochs}\\t loss: {train_loss_total/batch_num} Global: {tr_global_total/batch_num} Local: {tr_local_total/batch_num} Event: {tr_event_total/batch_num}\")\n",
    "  ph3_train_loss_log.append(train_loss_total/batch_num)\n",
    "  ph3_tr_global_log.append(tr_global_total/batch_num)\n",
    "  ph3_tr_local_log.append(tr_local_total/batch_num)\n",
    "  ph3_tr_event_log.append(tr_event_total/batch_num)\n",
    "\n",
    "\n",
    "  model.eval()\n",
    "  with torch.no_grad():   \n",
    "    batch_num = len(test_loader)\n",
    "    test_loss = 0\n",
    "    test_loss_total = 0\n",
    "    test_global_total = 0\n",
    "    test_local_total = 0\n",
    "    test_event_total = 0\n",
    "    with tqdm(test_loader, unit=\"batch\") as tepoch:\n",
    "      for data_batch in tepoch:\n",
    "        tepoch.set_description(f\"Test Epoch {epoch}\")\n",
    "        # Read in val batch\n",
    "        window_imgs, xy_downscale, _, event_probs = data_batch\n",
    "        window_batch = torch.as_tensor(window_imgs,dtype=torch.float32).to(device, non_blocking=True)\n",
    "        xy_downscale_batch=torch.stack((xy_downscale[0],xy_downscale[1])).transpose(0,1)\n",
    "\n",
    "        # Model forward step\n",
    "        with autocast():\n",
    "          global_out, local_out, _, crop_params, event_out = model(window_batch)\n",
    "\n",
    "        # Get event probs and local xy predictions\n",
    "        event_probs = torch.as_tensor(event_probs,dtype=torch.float32).to(device, non_blocking=True)\n",
    "        local_ball_xy = get_local_groundtruth(xy_downscale,crop_params)\n",
    "        local_ball_xy = torch.as_tensor(local_ball_xy,dtype=torch.float32).to(device)\n",
    "\n",
    "        # Calucate val (global+local+event) loss\n",
    "        with autocast():\n",
    "          global_loss_test = ball_loss(global_out,xy_downscale_batch) * global_weight\n",
    "          local_loss_test = ball_loss(local_out,local_ball_xy) * local_weight\n",
    "          event_loss_test = event_loss(event_out,event_probs) * event_weight\n",
    "          test_loss = global_loss_test + local_loss_test + event_loss_test\n",
    "        \n",
    "        loss = test_loss.detach().cpu().numpy()\n",
    "        glo = global_loss_test.detach().cpu().numpy()\n",
    "        local = local_loss_test.detach().cpu().numpy()\n",
    "        event = event_loss_test.detach().cpu().numpy()\n",
    "        \n",
    "        test_loss_total += loss\n",
    "        test_global_total += glo\n",
    "        test_local_total += local\n",
    "        test_event_total += event\n",
    "        tepoch.set_postfix(loss=loss)\n",
    "\n",
    "    ph3_test_loss_log.append(test_loss_total/batch_num)\n",
    "    ph3_test_global_log.append(test_global_total/batch_num)\n",
    "    ph3_test_local_log.append(test_local_total/batch_num)\n",
    "    ph3_test_event_log.append(test_event_total/batch_num)\n",
    "    tqdm.write(f\"Test\\t epoch: {epoch}/{ph3_epochs}\\t loss: {test_loss_total/batch_num} Global: {test_global_total/batch_num} Local: {test_local_total/batch_num} Event: {test_event_total/batch_num}\")\n",
    "\n",
    "  tqdm.write(\"Saving model\")\n",
    "  savePath = f\"{savePath_base}/Phase3/TTNet_Phase3_{epoch}.pth\"\n",
    "  state = {'model':model.state_dict(),'optimizer':optimizer.state_dict(),'lr_scheduler':lr_scheduler.state_dict(),'cur_epoch':epoch,'train_loss_log':ph3_train_loss_log,'val_loss_log':ph3_test_loss_log}\n",
    "  torch.save(state, savePath) \n",
    "  ph3_saved = True\n",
    "\n",
    "  # Plot the losses    \n",
    "  plt.clf()\n",
    "  plt.figure(dpi=1200)\n",
    "  plt.plot(range(1, epoch+1),ph3_train_loss_log,label='Train loss')\n",
    "  plt.plot(range(1, epoch+1),ph3_test_loss_log,label='Test loss')\n",
    "  plt.legend()\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.title('Training Loss')\n",
    "  plt.savefig(outputPath+\"loss_ph3.png\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks = [\"local\", \"event\"]\n",
    "model = TTNet(dropout_p=0.5, frame_window=9, threshold=0.01, tasks=tasks).to(device)\n",
    "savePath = savePath_base+\"Phase3/TTNet_Phase3_6.pth\"\n",
    "checkpoint = torch.load(savePath)\n",
    "model.load_state_dict(checkpoint['model']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing\n",
      "Frame 200\n",
      "Frame 400\n",
      "Frame 600\n",
      "Frame 800\n",
      "Frame 1000\n",
      "Frame 1200\n",
      "Frame 1400\n",
      "Frame 1600\n",
      "Loading demo dataset\n",
      "Making Predictions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc80d164c194a93a30f15438dde1deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/111 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transform = A.Compose([\n",
    "  A.Resize(height=TTN_height, width=TTN_width, interpolation=1, always_apply=True, p=1)],\n",
    "  additional_targets = {\n",
    "    'img2': 'image',\n",
    "    'img3': 'image',\n",
    "    'img4': 'image',\n",
    "    'img5': 'image',\n",
    "    'img6': 'image',\n",
    "    'img7': 'image',\n",
    "    'img8': 'image',\n",
    "    'img9': 'image',\n",
    "  })\n",
    "\n",
    "class demo_data_loader():\n",
    "  def __init__(self, window_size=9):\n",
    "    self.window_size = window_size\n",
    "    self.window_paths = []\n",
    "    self.jpeg_reader = TurboJPEG()\n",
    "    self.img_paths = sorted(glob.glob('./tmp/*.jpg'))\n",
    "\n",
    "    for frame_no, _ in enumerate(self.img_paths):\n",
    "      if not (window_size-2 < frame_no <= len(self.img_paths)-1):\n",
    "        continue\n",
    "\n",
    "      # Get frame window image paths\n",
    "      window_path = []\n",
    "      middle_index = frame_no-self.window_size+(self.window_size+1)//2\n",
    "      for window_frame in range(self.window_size):\n",
    "        window_path.append(self.img_paths[(frame_no+1)-self.window_size+window_frame])\n",
    "      self.window_paths.append(window_path)\n",
    "    \n",
    "  def loader(self, start_index):\n",
    "    # Get images\n",
    "    imgs = []\n",
    "    for i in range(9):\n",
    "      in_file = open(self.window_paths[start_index][i], 'rb')\n",
    "      image = self.jpeg_reader.decode(in_file.read(), 0)\n",
    "      imgs.append(image)\n",
    "    \n",
    "    # Apply transformation and get outputs\n",
    "    transformed = transform(image=imgs[0], img2=imgs[1], img3=imgs[2], img4=imgs[3], img5=imgs[4], img6=imgs[5], img7=imgs[6], img8=imgs[7], img9=imgs[8])\n",
    "    img1 = transformed['image']\n",
    "    img2 = transformed['img2']\n",
    "    img3 = transformed['img3']\n",
    "    img4 = transformed['img4']\n",
    "    img5 = transformed['img5']\n",
    "    img6 = transformed['img6']\n",
    "    img7 = transformed['img7']\n",
    "    img8 = transformed['img8']\n",
    "    img9 = transformed['img9']\n",
    "\n",
    "    return img1, img2, img3, img4, img5, img6, img7, img8, img9\n",
    "\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    img1, img2, img3, img4, img5, img6, img7, img8, img9 = self.loader(index)\n",
    "    window_imgs = np.concatenate((img1, img2, img3, img4, img5, img6, img7, img8, img9), axis=2)\n",
    "    window_imgs = np.rollaxis(window_imgs, 2, 0)\n",
    "    window_imgs = torch.from_numpy(window_imgs)\n",
    "    centre_path = self.window_paths[index][self.window_size//2]\n",
    "\n",
    "    return window_imgs, centre_path\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.window_paths)\n",
    "\n",
    "# Extracts all the frames from a video and saves them\n",
    "def preprocess(video_path):\n",
    "  cap = cv.VideoCapture(video_path)\n",
    "  ret, frame = cap.read()\n",
    "  count=0\n",
    "  while(ret):\n",
    "    cv.imwrite(f\"tmp/{count:04d}.jpg\", frame)\n",
    "    ret, frame = cap.read()\n",
    "    count += 1\n",
    "    if count %200 == 0:\n",
    "      print(f\"Frame {count}\")\n",
    "  cap.release\n",
    "\n",
    "# Predicts the ball location and events for a new video\n",
    "def predict(video_path, preprocessing=True):\n",
    "  if preprocessing:\n",
    "    print(\"Preprocessing\")\n",
    "    for f in os.listdir(\"tmp\"):\n",
    "      os.remove(f\"tmp/{f}\")\n",
    "    preprocess(video_path)\n",
    "  print(\"Loading demo dataset\")\n",
    "  demo_dataset  = demo_data_loader()\n",
    "  demo_loader = DataLoader(demo_dataset, batch_size=16,shuffle=False,num_workers=num_workers,pin_memory=True,drop_last=True)\n",
    "\n",
    "  cap = cv.VideoCapture(video_path)\n",
    "  fps = cap.get(cv.CAP_PROP_FPS)\n",
    "  cap.release()\n",
    "  fourcc = cv.VideoWriter_fourcc(*'MPEG')\n",
    "  out = cv.VideoWriter('./Results/videos/out.avi', fourcc, fps, (1920,1080))\n",
    "\n",
    "  print(\"Making Predictions\")\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    events = [\"Flying\", \"Bounce\", \"Hit\", \"Out of image\"]\n",
    "    preds = []\n",
    "    events = []\n",
    "\n",
    "    for i,data_batch in enumerate(tqdm(demo_loader)):\n",
    "      # Read in demo batch\n",
    "      window_imgs, centre_path = data_batch\n",
    "      window_batch = torch.as_tensor(window_imgs,dtype=torch.float32).to(device, non_blocking=True)\n",
    "      # Model forward step\n",
    "      global_out, local_out, local_in, crop_params, event_out = model(window_batch)\n",
    "\n",
    "      local_out_clone = local_out.clone().detach()\n",
    "      global_out_clone = global_out.clone().detach()\n",
    "      event_out_clone = event_out.clone().detach()\n",
    "\n",
    "      for out_index in range(local_out_clone.shape[0]):\n",
    "        # Global bounding box\n",
    "        _, x_min, y_min, x_max, y_max, _, _ = crop_params[out_index]\n",
    "\n",
    "        # Ball prediction\n",
    "        local_x = torch.argmax(local_out_clone[out_index,:TTN_width]).item()\n",
    "        local_y = torch.argmax(local_out_clone[out_index,TTN_width:]).item()\n",
    "        global_x = torch.argmax(global_out_clone[out_index,:TTN_width]).item()\n",
    "        global_y = torch.argmax(global_out_clone[out_index,TTN_width:]).item()\n",
    "\n",
    "        x_pred = global_x*data_width/TTN_width-TTN_width/2+local_x\n",
    "        y_pred = global_y*data_height/TTN_height-TTN_height/2+local_y\n",
    "        x_pred = 0 if x_pred<0 else int(x_pred)\n",
    "        y_pred = 0 if y_pred<0 else int(y_pred)\n",
    "        preds.append([x_pred, y_pred])\n",
    "\n",
    "        # Event prediction\n",
    "        event_preds = event_out_clone[out_index]\n",
    "        events.append([event_preds[0].item(), event_preds[1].item(), event_preds[2].item(), event_preds[3].item()])\n",
    "    \n",
    "        # Label frame\n",
    "        centre_img = cv.imread(centre_path[out_index])\n",
    "        \n",
    "        # Draw the prediction if the ball is in the frame\n",
    "        if event_preds[3].item() < 0.6:\n",
    "          cv.rectangle(centre_img, (x_min,y_min), (x_max,y_max), (0,0,255), 2)\n",
    "          cv.circle(centre_img,(x_pred,y_pred), 8,  (0, 0, 255), 2)\n",
    "        \n",
    "        if event_preds[0].item() > 0.8:\n",
    "          event_text = f\"Event: Flying {event_preds[0].item():.2f}\"\n",
    "          cv.putText(centre_img, event_text, (50, 100), cv.FONT_HERSHEY_SIMPLEX, 1.0, (255, 0, 0), 2)\n",
    "        if event_preds[1].item() > 0.5:\n",
    "          event_text = f\"Event: Bounce {event_preds[1].item():.2f}\"\n",
    "          cv.putText(centre_img, event_text, (50, 150), cv.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2)\n",
    "        if event_preds[2].item() > 0.5:\n",
    "          event_text = f\"Event: Hit {event_preds[2].item():.2f}\"\n",
    "          cv.putText(centre_img, event_text, (50, 200), cv.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 2)\n",
    "        if event_preds[3].item() > 0.6:\n",
    "          event_text = f\"Event: Out of image {event_preds[3].item():.2f}\"\n",
    "          cv.putText(centre_img, event_text, (50, 250), cv.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 0), 2)\n",
    "\n",
    "        out.write(centre_img)\n",
    "  out.release()\n",
    "  return preds, events\n",
    "\n",
    "# Set preprocessing to True the first time you predict a video\n",
    "preds, events = predict(\"./Dataset/videos/test2.mp4\", preprocessing=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('ttn-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "977ae9768b98c0a6351747d91b3b0f777616684af223aba46050bfee78a134e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
